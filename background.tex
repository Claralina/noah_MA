\chapter{Background}\label{Background}
This chapter aims to provide a comprehensive overview of the key concepts, challenges, and advancements in privacy-preserving computation, specifically \ac{SMPC}, and record linkage.
\section{Secure Multi-Party Computation}

\subsection{Adversary Models}
\subsection{Yao's Garbled Circuits}
\subsection{Arithmetic Sharing}
\subsection{Boolean Sharing}
\section{Record Linkage}
\subsection{Concept and Application}
Record linkage is the process of matching data points (records) related to the same entity, i.e. human, but originating from different datasets.
If a commonly shared unique identifier exists, this process is trivial. % TODO find example of this
For instance, the \ac{SSN} in the United States of America, in theory, serves this purpose.
In practice, even this is not perfect due to erroneous data.
Unique identifiers may have spelling mistakes or typos, so that a match is no longer possible.
More importantly, there are many instances where such a unique identifier is not readily available.
In these instances, the matching must be done on features of the entity that are found in both datasets. % TODO find multiple areas where record linkage is employed
The term record linkage in its broadest definition may be used to refer to any such process of linking datasets, such as %TODO find example (Arjun mentioned something?)
In this work, the term record linkage pertains to the linking of datasets using \ac{IDAT}.
These include information such as name, date of birth and place of residence of a patient.
In medical research, record linkage is commonly employed to match patient data from various healthcare institutions, creating a dataset that combines information from different sources.
Individually, each \ac{IDAT} point is insufficient to provide an unambiguous match, which is why they are often referred to as quasi-identifiers. % TODO where?
However, when combining enough quasi-identifiers, such a match can be achieved.
Issues in record linkage based on \ac{IDAT} arise when the data is either non-unique or inaccurate, which leads us to error types and their sources.

\Ac{IDAT} inaccuracies arise through various means, primarily during manual data entry.
Human input, typically via keyboard, introduces errors stemming from multiple sources.
Spelling mistakes may result from transcribing orally received information or copying text-based information.
Oral communication may introduce discrepancies due to similar-sounding words or varied spellings of the same name.
Copying text can lead to confusion between visually similar letters (e.g., I and l).
Typographical errors, induced by slips during typing, contribute to insertions, substitutions, transpositions, or deletions of letters.
Lastly, misplacement of individual attributes, such as swapping a patient's first name and surname, represents another source of error.

% TODO add section discussing research of spelling mistakes etc

\subsection{Approaches}
To tackle the complexities of record linkage, three main classes of record linkage have emerged:
Rule-based (deterministic) record linkage, probabilistic record linkage and more advanced forms using machine learning classifiers.

Deterministic record linkage describes the procedures where a binary outcome is obtained.
Records are either classified as matching or not matching.
Because this approach requires high data quality due to its inability to deal with errors, it is not reported often.
One example where such an approach has been used appears to be Surveillance, Epidemiology and End Results-Medicare linked dataset created by the US National Cancer Institute as reported by Dusetzina et al. \cite{dusetzinaOverviewRecordLinkage2014}, though the primary sources reporting this are not available anymore.
Here, records were first matched on the \ac{SSN} and combinations of first and last name, month of birth and sex.
Then, a second round of linkage is carried out for records without a \ac{SSN} or which were not matched in the previous round.
In this round, first name, last name, month of birth and sex are matched, in addition to either a part of the \ac{SSN} or a combination of day of birth, year of birth, middle initial and date of death.
It is noteworthy that despite being an example of deterministic record linkage, the first and last name allow for fuzzy matches to account for nicknames.
This is an example of data preprocessing, which will be described in more detail later.

Probabilistic record linkage is a more lenient approach in that rather than matching records based on hard rules, a match probability or record similarity metric is computed for pairs of records.
This metric is subsequently thresholded to categorise record pairs into matches and non-matches.
Probabilistic record linkage is generally better for dealing with low quality data with many errors.
It has the benefit of being flexible in weighing up specificity against sensitivity.
Namely, a researcher requiring very high sensitivity may set a low threshold to catch as many true matches as possible, at the detriment of increasing false positive matches.
In contrast, a researcher interested only in the highest quality matches may set a high threshold, thus filtering out false positives at the cost of sensitivity.
There are a number of algorithms employing probabilistic record linkage.

Perhaps the most classical example of a probabilistic record linkage model is the Fellegi-Sunter model \cite{fellegiTheoryRecordLinkage1969a}.
To compute the probability of a  match between a given pair of records, Fellegi and Sunter start with a prior probability that two randomly drawn records match.
Each record attribute is compared individually and is given a partial match weight based on how similar the attribute is between the two records.
The sum of all match weights is used together with the prior probability to compute a specific probability that the two records match.
Each partial match weight is estimated via $m$ and $u$ probabilities.
The relation between an attribute pair may be match/no match or a similarity metric, depending on the specific implementation of the Fellegi-Sunter model.
$m$ is the probability of observing the attribute pair relation on hand given the two records are a match.
Likewise, $u$ is the probability of the relation given the two records are not a match.
$m$ and $u$ can be computed in various ways, either from the given datasets themselves or from prior knowledge about the populations the records are drawn from.
When implemented well, $m$ and $u$ will incorporate information about error rates of an attribute as well as individual frequencies of the given attributes.
Since Fellegi and Sunter only describe a general mathematical framework, the actual result depends on the specific implementation and probability computation, of which Fellegi and Sunter give a few suggestions.
\subsection{Blocking}
In the most naive implementation of record linkage, each record in set $A$ has to be compared to each record in set $B$, giving a quadratic complexity of $\vert A\vert \times \vert B \vert$ comparisons.
To reduce this number, \textit{blocking} is employed.
Here, records are grouped into blocks by differing criteria, so only those records within each block have to be compared.
In its most basic form, blocks may simply be defined based on a specific attribute such as the city or year of birth.
This technique is extremely simple to implement, but it has a significant downside
-- if there is an error such as a misspelling or typo in a record's attribute selected for blocking, it becomes impossible to match this record.
Therefore, it is only appropriate if the researcher can be certain a specific attribute is relatively error-free across all records.
Therefore, more advanced blocking techniques have been developed.
Blocks may be defined by clustering the records.

Clustering is the process of dividing data points into groups such that within-variance of each group is minimised, while between-variance between groups is maximised.
This variance is computed based on a distance or similarity function.
Many such functions exist -- distance functions include the Pearson correlation distance, the standardised Euclidean distance and the Minkowski distance;
for similarity functions, these may be the Jaccard similarity or the Hamming similarity.
Clustering algorithms can be divided into nine categories according to Xu and Tian \cite{xuComprehensiveSurveyClustering2015}.
These categories describe what the algorithm is based on.
Partition-based algorithms
One such algorithm is K-means clustering, where $k$ cluster centers are defined.
Data points are assigned to the cluster with the nearest center, which is then updated to include the data point \cite{lloydLeastSquaresQuantization1982}.
Hierarchy-based algorithms try to establish a hierarchy of clusters.
This is either achieved bottom-up or top-down.
In bottom-up algorithms such as Clink \cite{defaysEfficientAlgorithmComplete1977}, the algorithm starts with each data point having its own cluster.
Iteratively, the two closest clusters are merged until just a single one remains.
In contrast, top-down algorithms such as DIANA \cite{kaufmanDivisiveAnalysisProgram2009} start with a single cluster and iteratively split each cluster into two to create the hierarchy.
\ac{FCM} clustering \cite{dunnFuzzyRelativeISODATA1973} is an example of a fuzzy theory-based algorithm.
These algorithms use fuzzy cluster membership, so that a single data point may belong to multiple clusters and cluster membership is not binary but a continuous number between zero and one.
\Ac{FCM} clustering otherwise works similarly to k-means clustering.
The other clustering categories are distribution-based, density-based, graph theory-based, grid-based, fractal theory-based and model-based clustering \cite{xuComprehensiveSurveyClustering2015}.
When using a clustering technique as a precursor to record linkage, it is important to choose one that does not have a higher complexity than the record linkage itself.
The aforementioned hierarchical clustering algorithm Clink, for example, has a time complexity of $O(n^2)$ \cite{defaysEfficientAlgorithmComplete1977}, which is not an improvement in comparison to naive record linkage.
On the other hand, clustering methods with a time complexity of $O(n)$, such as the hierarchical clustering algorithm BIRCH \cite{zhangBIRCHEfficientData1996}, may greatly increase the runtime of record linkage when used for blocking.

% TODO introduce section about data preprocessing
% TODO describe blocking
\subsection{Privacy-Preserving Techniques}
Privacy regulations such as the European Union's regulation on the protection of personal data % TODO cite
 or Health Insurance Portability and Accountability Act of the USA restrict disclosing sensitive personal data. % TODO cite
These restrictions necessitate algorithms for record linkage that do not require the exchange of \ac{IDAT}.
To accommodate these requirements, \ac{PPRL} algorithms have been developed.
Generally, these algorithms encode \ac{IDAT} so that the encoded versions can still be compared but do not reveal information about the original data.
This may be done via a hashing function such as MD5. %TODO cite
However, with hashes, only a binary match can be obtained, which can only identify string equivalence, not similarity.
The reason for this is that similarity measures between two differing hashes do not correlate with the similarity of the strings from which the hashes were derived.
Therefore, to the author's knowledge there is no \ac{PPRL} algorithm employing hashes only.
Hashing-based algorithms can also fall prey to \textit{dictionary attacks} - given a hash, an adversary can compute hashes for many different words until they find the word that produces a given hash.
To remedy this,a \ac{HMAC} may be used.
Here, a secret key is exchanged between the parties involved in \ac{PPRL}, and two padding strings $ipad$ and $opad$ of length $B$ are defined.
When a party needs to compute a hash, the following procedure takes place:

The secret key is padded with zeros to the length $B$.
Two \ac{XOR} computations are carried out -- between the padded secret key and either $ipad$ or $opad$.
Then, the string to be hashed is appended to the $ipad$ \ac{XOR} result, and the hashing function is applied to the resulting string.
The $opad$ \ac{XOR} result is appended to the hash, and the hashing function is applied to this string to obtain the final result.

The final hash cannot be replicated without knowing the secret key, rendering dictionary attacks void.
This technique was originally developed to authenticate messages, where the hash was computed from the original message
\cite{krawczykHMACKeyedHashingMessage1997}.

An extension of the concept of hashes is using Bloom filters. % TODO include figure showing Bloom filters
Bloom filters were originally invented to easily test whether an item is a member of a set.
A Bloom filter consists of a zero-initialised bit array of length $n$.
For each item in the set, $l$ hashes are computed using hash functions that have an output range of $n$ values.
For each computed hash $h$, the $h$th bit in the bit array is set to 1.
To test an item for membership of the set, one can simply compute the $l$ hashes of the item and see if the Bloom filter was set to 1 in all $l$ positions given by the hashes.
This method has a false negative error rate of 0\%, though its false positive error rate is larger.
Since Bloom filter-based \ac{PPRL} algorithms also rely on hashes, \ac{HMAC}s are required to prevent dictionary attacks.
Additionally, Bloom filters are weak to \textit{frequency attacks}. % TODO elaborate and explain frequency attacks

One \ac{PPRL} algorithm employing Bloom filters was proposed by Schnell et al. % TODO cite and explain in detail
\section{PPRL protocols employing SMPC}
\cite{lazrigPrivacyPreservingProbabilistic2018} % TODO explain and contrast
\cite{laudPrivacypreservingRecordLinkage2018} % TODO explain and contrast
\cite{chenPerfectlySecureEfficient2018} % TODO explain and contrast
\subsection{Mainzelliste SecureEpiLinker}
To the author's knowledge, \ac{MainSEL}, developed by Stammler et al.~\cite{stammlerMainzellisteSecureEpiLinkerMainSEL2022}, is the most recent example of an \ac{PPRL} algorithm utilising \ac{SMPC}.
The tool was developed to integrate with the Mainzelliste software, which is already in use.~\cite{lablansRESTfulInterfacePseudonymization2015} % TODO look up where and how
Leveraging the \ac{ABY} framework~\cite{demmlerABYFrameworkEfficient2015}, the tool facilitates the conversion between various \ac{SMPC} protocols.
This section will initially outline \ac{MainSEL}'s record linkage algorithm and subsequently detail its encoding within \ac{SMPC} protocols.

In \ac{MainSEL}'s record linkage computation, two primary functions are employed.
The initial function calculates the pairwise match scores between records, while the second function processes the match scores to yield pairs of records that are linked. % TODO make sure this is what I meant to write

Each attribute in a record is assigned a weight reflective of its significance for record linkage, derived from observed error rates and frequencies. MainSEL computes a match score by evaluating the similarity score $sim$ for each attribute. For numeric attributes and those intolerant to errors, $sim$ is a binary value $\in {0, 1}$ indicating field equality. For string attributes allowing errors such as typos, similarity is determined using Bloom filters. To obtain the Bloom filters, 15 hashes uniformly distributed within the range $0 \le h < p$ are computed for each bigram.

For optimisation purposes, the 15 hashes are obtained from just two hashing functions $h_1$ and $h_2$,
by computing:
\[g_i(x)=h_1(x)+ih_2(x) \mod p; i \in \mathbb{Z}; 0 \le i < 15\]
, as described by Kirsch and Mitzenmacher. % TODO cite Kirsch and Mitzenmacher, 2006
In a zero-initialised bit array of size $p$, the \textit{Bloom filter}, the value at the $h^{th}$ index is set to 1 for each hash $h$.
This is done for the field from both records, $x$ and $y$, to obtain the Bloom filters $Bl(x)$ and $Bl(y)$.
Bloom filter similarity is given by the Dice score.
%TODO reference section that mentions Dice scores and cites Dice, 1945

The score of the overall IDAT similarity is the normalised weighted sum of field similarities:
\[S(x, y)=\frac{\sum\limits_{i\in I}\delta_{i,i}w_{isim_i(x_i, y_i)}}{\sum\limits_{i\in I}\delta_{i,i}w_i}\]
, ranging from 0 to 1.
The numerator of this fraction will henceforth be referred to as $s(x, y)$, and the denominator as $w(x, y)$.

Record attributes prone to accidental swaps, such as first names and surnames, are organized into \textit{exchange groups}.
Within each exchange group, the similarity score is computed for all permutations of pairwise attribute combinations.
For instance, in the case of first names and surnames, this involves pairing both first names and both surnames with each other, as well as the first name of record 1 with the surname of record 2 and vice versa. The highest achieved similarity score across all exchange group permutations is selected as the overall score of the exchange group. % TODO add section about exchange group weights

Since many similarity score comparisons need to be carried out in the process of record linkage and division is expensive in \ac{SMPC}, Stammler et al.\ defined a comparison of two scores that does not require calculating $(s_1/w_1) > (s_2/w_2)$ and includes a tie-solver in the case that the values are identical:
\[(s_1, w_1) > (s_2, w_2) :\iff (s_1 w_2>s_2 w_1) \lor (s_1 w_2 = s_2 w_1 \land w_1 > w_2)\]

Executing the complete record linkage protocol with \ac{MainSEL} necessitates a minimum of two instances of Mainzelliste, each containing patient data from their respective institutions, associated \ac{MainSEL} instances, and a linkage service.
To enable checking for matches in another institution's records without requiring patient consent, the linked records cannot be transmitted to the \ac{MainSEL} instances in clear text.
If this is ensured, record linkage can be performed proactively.
Patients' consent is only necessary if the desired number of matches are obtained and the linked records are retrieved.
The linkage service plays a crucial role in facilitating this functionality.

During the initialization of record linkage, MainSEL establishes a local connection with Mainzelliste.
Subsequently, configurations for communication with remote MainSEL instances are set, and encrypted connections are established for each.
Successful connection prompts a test to confirm the correctness of configurations before initiating the record linkage.
Mainzelliste starts the process by transmitting one or more records to MainSEL (referred to as SEL1), which sends the record count to the remote MainSEL instance (SEL2).
This information is essential for the offline phase, where the circuit is constructed.
Following circuit construction, the record linkage computation occurs, as detailed in section 3.
Upon completion, SEL1 transmits its share of the results (indices of matched records and a boolean mask indicating matches) to the linkage service.
SEL2 also sends its shares, enabling the linkage service to reconstruct the results.
The linkage service encrypts the matches' IDs and transmits only the encrypted linkage ID (LID) and the number of matches back to SEL1.
Patient consent for data sharing triggers SEL1 to send the LID to the linkage service, which decrypts it and forwards the decrypted information to SEL1 and SEL2.

Stammler et al. opted to encode floating-point numbers in fixed-point representation. % TODO why do SMPC protocols use fixed point representation?
They store the value in $C$, a single bit vector of length $L$.
Given that field similarities $sim$ fall within the range of 0 to 1, the floating-point value $C(sim)$ with precision $l_s$ can be expressed as $C(sim)=2^{l_s}sim$. The field weights, known beforehand, can be rescaled such that the highest weight under precision $l_w$ equals $2^{l_w}-1$.
The comparison of similarity scores involves multiplying sums of $n$ $sim$ values and $n$ $w$ values.
Therefore, if the result should not overflow $C$, $l_s$ and $l_w$ are dependent on $L$ and the number of fields $n$.
\ac{ABY} allows $L$ to be set as 8, 16, 32, or 64.
In comparison to double floating-point precision, when $n=8$, Stammler et al. observed deviations of $<1\%$, $<0.1\%$, and a negligible amount for $L=16$, $L=32$, and $L=64$ respectively.

The two calculations to compute  are split into three circuits, \texttt{C1}, \texttt{C2} and \texttt{C3}.
Stammler et al.\ implemented the circuits in all three sMPC protocols,
i.e.\ Yao's garbled circuits (Y), additive sharing (A) and boolean sharing (B).
For boolean components such as equality evaluations ($\beta$), either B or Y can be used.
Arithmetic components ($\alpha$) may use A, B or Y\@.
Given a record $x$ and a set of records $y^i$,
\begin{itemize}
    \item\texttt{C1} computes $s(x, y^i)$ and $w(x, y^i)$ for all $i$
    \item\texttt{C2} computes the $\argmax$ and $\max$ from the output of \texttt{C1}
    \item\texttt{C3} computes the \textit{match bit}, i.e.\ whether the resulting score from \texttt{C2} reaches/exceeds the threshold for matches.
\end{itemize}
In \texttt{C1}, two distinct similarity circuits operating in $\beta$ calculate $sim$ scores.
For fields allowing only exact matches, a straightforward equality check followed by a bit shift, as outlined earlier, is sufficient.
Fields permitting inexact matches employ the Bloom filter similarity method described previously.
The locally computed Bloom filters serve as inputs to the circuit.
To compute the similarity score for exchange groups $s_G$, the function \texttt{MaxQuotient} identifies the maximum $s$ among all permutations within each exchange group.
This involves pairwise comparisons in mixed protocols ($\alpha$ and $\beta$), iterating through the list and storing the largest value for subsequent pairwise comparisons to retrieve the maximum.

\texttt{maxQuotient} is also utilized in \texttt{C2}, where it helps obtain the $\max$ and $\argmax$ of all $s(x, y^i)$.

Ultimately, \texttt{C3} takes the output of \texttt{C2} and assesses $s>Tw$.
