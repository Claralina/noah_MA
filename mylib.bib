@article{defaysEfficientAlgorithmComplete1977,
  title = {An Efficient Algorithm for a Complete Link Method},
  author = {Defays, D.},
  year = {1977},
  month = jan,
  journal = {The Computer Journal},
  volume = {20},
  number = {4},
  pages = {364--366},
  issn = {0010-4620},
  doi = {10.1093/comjnl/20.4.364},
  urldate = {2024-01-06},
  abstract = {An improved algorithm for a complete linkage clustering is discussed. The algorithm is based, like the algorithm for the single link cluster method (Slink) presented by Sibson (1973), on a compact representation of a dendrogram: the pointer representation. This approach offers economy in computation. The algorithm is easily programmable.},
  file = {/home/noah/Zotero/storage/HIXEULRS/Defays - 1977 - An efficient algorithm for a complete link method.pdf;/home/noah/Zotero/storage/P3PCCN4I/393966.html}
}

@article{dunnFuzzyRelativeISODATA1973,
  title = {A {{Fuzzy Relative}} of the {{ISODATA Process}} and {{Its Use}} in {{Detecting Compact Well-Separated Clusters}}},
  author = {Dunn, J. C.},
  year = {1973},
  month = jan,
  journal = {Journal of Cybernetics},
  volume = {3},
  number = {3},
  pages = {32--57},
  publisher = {{Taylor \& Francis}},
  issn = {0022-0280},
  doi = {10.1080/01969727308546046},
  urldate = {2024-01-06},
  abstract = {Two fuzzy versions of the k-means optimal, least squared error partitioning problem are formulated for finite subsets X of a general inner product space. In both cases, the extremizing solutions are shown to be fixed points of a certain operator T on the class of fuzzy, k-partitions of X, and simple iteration of T provides an algorithm which has the descent property relative to the least squared error criterion function. In the first case, the range of T consists largely of ordinary (i.e. non-fuzzy) partitions of X and the associated iteration scheme is essentially the well known ISODATA process of Ball and Hall. However, in the second case, the range of T consists mainly of fuzzy partitions and the associated algorithm is new; when X consists of k compact well separated (CWS) clusters, Xi , this algorithm generates a limiting partition with membership functions which closely approximate the characteristic functions of the clusters Xi . However, when X is not the union of k CWS clusters, the limiting partition is truly fuzzy in the sense that the values of its component membership functions differ substantially from 0 or 1 over certain regions of X. Thus, unlike ISODATA, the ``fuzzy'' algorithm signals the presence or absence of CWS clusters in X. Furthermore, the fuzzy algorithm seems significantly less prone to the ``cluster-splitting'' tendency of ISODATA and may also be less easily diverted to uninteresting locally optimal partitions. Finally, for data sets X consisting of dense CWS clusters embedded in a diffuse background of strays, the structure of X is accurately reflected in the limiting partition generated by the fuzzy algorithm. Mathematical arguments and numerical results are offered in support of the foregoing assertions.}
}

@incollection{dusetzinaOverviewRecordLinkage2014,
  title = {An {{Overview}} of {{Record Linkage Methods}}},
  booktitle = {Linking {{Data}} for {{Health Services Research}}: {{A Framework}} and {{Instructional Guide}} [{{Internet}}]},
  author = {Dusetzina, Stacie B. and Tyree, Seth and Meyer, Anne-Marie and Meyer, Adrian and Green, Laura and Carpenter, William R.},
  year = {2014},
  month = sep,
  publisher = {{Agency for Healthcare Research and Quality (US)}},
  urldate = {2024-01-06},
  abstract = {Randomized controlled trials (RCTs) remain the gold standard for assessing intervention efficacy; however, RCTs are not always feasible or sufficiently timely. Perhaps more importantly, RCT results often cannot be generalized due to a lack of inclusion of ``real-world'' combinations of interventions and heterogeneous patients.1{\textendash}4 With recent advances in information technology, data, and statistical methods, there is tremendous promise in leveraging ever-growing repositories of secondary data to support comparative effectiveness and public health research.2,6,39 While there are many advantages to using these secondary data sources, they are often limited in scope, which in turn limits their utility in addressing important questions in a comprehensive manner. These limitations can be overcome by linking data from multiple sources such as health registries and administrative claims data.7,14{\textendash}16},
  langid = {english},
  file = {/home/noah/Zotero/storage/TWTWIPH6/NBK253312.html}
}

@article{fellegiTheoryRecordLinkage1969a,
  title = {A {{Theory}} for {{Record Linkage}}},
  author = {Fellegi, Ivan P. and Sunter, Alan B.},
  year = {1969},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {64},
  number = {328},
  pages = {1183--1210},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1969.10501049},
  urldate = {2024-01-06},
  abstract = {Abstract A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A 1), a non-link (A 3), and a possible link (A 2). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A 1 when the members of the comparison pair are in fact unmatched, and the error of the decision A 3 when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as and respecti...},
  langid = {english}
}

@inbook{kaufmanDivisiveAnalysisProgram2009,
  title = {6. {{Divisive Analysis}} ({{Program DIANA}})},
  booktitle = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  year = {2009},
  month = sep,
  pages = {253--279},
  publisher = {{John Wiley \& Sons}},
  collaborator = {Kaufman, Leonard and Rousseeuw, Peter J.},
  googlebooks = {YeFQHiikNo0C},
  isbn = {978-0-470-31748-8},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{kaufmanFindingGroupsData2009a,
  title = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  shorttitle = {Finding {{Groups}} in {{Data}}},
  author = {Kaufman, Leonard and Rousseeuw, Peter J.},
  year = {2009},
  month = sep,
  publisher = {{John Wiley \& Sons}},
  abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "Cluster analysis is the increasingly important and practical subject of finding groupings in data. The authors set out to write a book for the user who does not necessarily have an extensive background in mathematics. They succeed very well."{\textemdash}Mathematical Reviews "Finding Groups in Data [is] a clear, readable, and interesting presentation of a small number of clustering methods. In addition, the book introduced some interesting innovations of applied value to clustering literature."{\textemdash}Journal of Classification "This is a very good, easy-to-read, and practical book. It has many nice features and is highly recommended for students and practitioners in various fields of study."{\textemdash}Technometrics An introduction to the practical application of cluster analysis, this text presents a selection of methods that together can deal with most applications. These methods are chosen for their robustness, consistency, and general applicability. This book discusses various types of data, including interval-scaled and binary variables as well as similarity data, and explains how these can be transformed prior to clustering.},
  googlebooks = {YeFQHiikNo0C},
  isbn = {978-0-470-31748-8},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@techreport{krawczykHMACKeyedHashingMessage1997,
  type = {Request for {{Comments}}},
  title = {{{HMAC}}: {{Keyed-Hashing}} for {{Message Authentication}}},
  shorttitle = {{{HMAC}}},
  author = {Krawczyk, Hugo and Bellare, Mihir and Canetti, Ran},
  year = {1997},
  month = feb,
  number = {RFC 2104},
  institution = {{Internet Engineering Task Force}},
  doi = {10.17487/RFC2104},
  urldate = {2023-12-28},
  abstract = {This document describes HMAC, a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key. The cryptographic strength of HMAC depends on the properties of the underlying hash function. This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind},
  file = {/home/noah/Zotero/storage/6427DMFG/Krawczyk et al. - 1997 - HMAC Keyed-Hashing for Message Authentication.pdf}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2024-01-06},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\^bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {/home/noah/Zotero/storage/VDWZQLCQ/Lloyd - 1982 - Least squares quantization in PCM.pdf;/home/noah/Zotero/storage/YJUBSNGG/1056489.html}
}

@article{schmidtmannQualityRecordLinkage2016,
  title = {{Quality of record linkage in a highly automated cancer registry that relies on encrypted identity data}},
  author = {Schmidtmann, Irene and Sariyar, Murat and Borg, Andreas and {Gerold-Ay}, Aslihan and Heidinger, Oliver and Hense, Hans-Werner and Krieg, Volker and Hammer, Ga{\"e}l Paul},
  year = {2016},
  month = jun,
  journal = {GMS Medizinische Informatik, Biometrie und Epidemiologie},
  volume = {12},
  number = {1},
  pages = {Doc02},
  publisher = {{German Medical Science GMS Publishing House}},
  issn = {1860-9171},
  doi = {10.3205/mibe000164},
  abstract = {Ziel der Arbeit: Krankheitsregister in Deutschland und einigen anderen L{\"a}ndern sind darauf angewiesen, mehrere Meldungen zu einem Patienten anhand von Identit{\"a}tsdaten zusammenzuf{\"u}hren, da keine eindeutige Personenkennung verf{\"u}gbar ist. Diese Identit{\"a}tsdaten werden h{\"a}ufig aus Datenschutzgr{\"u}nden pseudonymisiert. Einige Fehler beim Record Linkage sind unvermeidbar. In der vorliegenden Arbeit werden sie f{\"u}r das Epidemiologische Krebsregister Nordrhein-Westfalen, das chiffrierte Identit{\"a}tsdaten zum Record Linkage verwendet, quantifiziert. Methoden: Eine Stichprobe von Meldungen an das Epidemiologische Krebsregister Nordrhein-Westfalen, die mit der Information {\"u}ber die Zuordnung zu einer Person versehen war, wurde gezogen. Parallel dazu wurden Klartextidentit{\"a}tsdaten f{\"u}r diese Meldungen durch Dechiffrierung wiedergewonnen, um einen Gold-Standard zu erzeugen. Die Zuordnungsfehlerh{\"a}ufigkeiten wurden durch Vergleich der Ergebnisse des Routine-Record Linkage mit dem Gold-Standard bestimmt. Die Fehlerraten wurden f{\"u}r umfangreichere Register hochgerechnet.Ergebnisse: In der untersuchten Stichprobe betrug die Homonymfehlerrate 0.015\%, die Synonymfehlerrate 0.2\%, das F-Ma{\ss} ergab 0.9921. Eine Hochrechnung auf umfangreichere Datenbanken ergab, dass bei realistischen Annahmen {\"u}ber die Zunahme des Umfangs eine Homonymfehlerrate von etwa 1\% und eine Synonymfehlerrate von etwa 2\% zu erwarten sind.Schlussfolgerung: Die beobachteten Fehlerraten sind niedrig, darin zeigt sich, dass effektive Methoden zur Standardisierung und Sicherung der Datenqualit{\"a}t implementiert wurden. Dies ist essenziell, damit die Fehlerraten niedrig bleiben, wenn der Umfang des Registers zunimmt. Durch den geplanten Einschluss der eindeutigen Krankenversicherungsnummer ist eine weitere Verbesserung der G{\"u}te des Record Linkage zu erwarten. Krebsregistrierung, die allein auf elektronische Meldungen basiert, kann gro{\ss}e Datenmengen verarbeiten bei hoher Qualit{\"a}t des Record Linkage.},
  copyright = {This is an Open Access article distributed under the terms of the Creative Commons Attribution 4.0 License.},
  langid = {engl},
  keywords = {cancer registry,data quality,encrypted data,evaluation,record linkage},
  file = {/home/noah/Zotero/storage/FGJSQSSA/Schmidtmann et al. - 2016 - Quality of record linkage in a highly automated ca.pdf;/home/noah/Zotero/storage/9BPDKQRJ/mibe000164.html}
}

@inproceedings{tranGeCoOnlinePersonal2013,
  title = {{{GeCo}}: An Online Personal Data Generator and Corruptor},
  shorttitle = {{{GeCo}}},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Information}} \& {{Knowledge Management}}},
  author = {Tran, Khoi-Nguyen and Vatsalan, Dinusha and Christen, Peter},
  year = {2013},
  month = oct,
  series = {{{CIKM}} '13},
  pages = {2473--2476},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2505515.2508207},
  urldate = {2023-10-12},
  abstract = {We demonstrate GeCo, an online personal data GEnerator and COrruptor that facilitates the creation of realistic personal data ranging from names, addresses, and dates, to social security and credit card numbers, as well as numerical values such as salary or blood pressure. Using an intuitive Web interface, a user can create records containing such data according to their needs, and apply various corruption functions to generate duplicates of these records. Synthetic personal data are increasingly required in areas such as record de-duplication, fraud detection, cloud computing, and health informatics, where data quality issues can significantly affect the outcomes of data integration, processing, and mining projects. Privacy concerns, however, often make it difficult for researchers to obtain real data that contain personal details. Compared to other data generators that have to be downloaded, installed and customized,GeCo allows the creation of personal data with much less effort. In this demonstration we show (1) how different types of attributes, and dependencies between them, can be specified; (2) how the generated data can be modified using various types of corruption functions; and (3) how a user can contribute to GeCo by providing attribute generation functions and look-up files. We believe GeCo will be a valuable tool for researchers that require realistic personal data to evaluate their algorithms with regard to efficiency and effectiveness.},
  isbn = {978-1-4503-2263-8},
  keywords = {data generation,duplicates,online demo,synthetic data},
  file = {/home/noah/Zotero/storage/GLIF8LI2/Tran et al. - 2013 - GeCo an online personal data generator and corrup.pdf}
}

@article{xuComprehensiveSurveyClustering2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  year = {2015},
  month = jun,
  journal = {Annals of Data Science},
  volume = {2},
  number = {2},
  pages = {165--193},
  issn = {2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  urldate = {2024-01-06},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table~22.},
  langid = {english},
  keywords = {Clustering,Clustering algorithm,Clustering analysis,Survey,Unsupervised learning},
  file = {/home/noah/Zotero/storage/RM2AC5JB/Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf}
}

@inproceedings{zhangBIRCHEfficientData1996,
  title = {{{BIRCH}}: An Efficient Data Clustering Method for Very Large Databases},
  shorttitle = {{{BIRCH}}},
  booktitle = {Proceedings of the 1996 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
  year = {1996},
  month = jun,
  series = {{{SIGMOD}} '96},
  pages = {103--114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/233269.233324},
  urldate = {2024-01-06},
  abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
  isbn = {978-0-89791-794-0},
  file = {/home/noah/Zotero/storage/28BULLJU/Zhang et al. - 1996 - BIRCH an efficient data clustering method for ver.pdf}
}
