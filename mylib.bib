@article{chenPerfectlySecureEfficient2018,
  title = {Perfectly {{Secure}} and {{Efficient Two-Party Electronic-Health-Record Linkage}}},
  author = {Chen, Feng and Jiang, Xiaoqian and Wang, Shuang and Schilling, Lisa M. and Meeker, Daniella and Ong, Toan and Matheny, Michael E. and Doctor, Jason N. and {Ohno-Machado}, Lucila and Vaidya, Jaideep},
  year = {2018},
  month = mar,
  journal = {IEEE Internet Computing},
  volume = {22},
  number = {2},
  pages = {32--41},
  issn = {1941-0131},
  doi = {10.1109/MIC.2018.112102542},
  urldate = {2024-01-07},
  abstract = {Patient health data are often found spread across various sources. However, precision medicine and personalized care require access to the complete medical records. The first step toward this is to enable the linkage of health records spread across different sites. Existing record linkage solutions assume that data are centralized with no privacy or security concerns restricting sharing. However, that is often untrue. Therefore, we have designed and implemented a portable method for privacy-preserving record linkage based on garbled circuits to accurately and securely match records. We have also developed a novel approximate-matching mechanism that significantly improves efficiency.},
  file = {/home/noah/Zotero/storage/ALXCL8CD/Chen et al. - 2018 - Perfectly Secure and Efficient Two-Party Electroni.pdf;/home/noah/Zotero/storage/NI8X8S62/8259429.html}
}

@article{defaysEfficientAlgorithmComplete1977,
  title = {An Efficient Algorithm for a Complete Link Method},
  author = {Defays, D.},
  year = {1977},
  month = jan,
  journal = {The Computer Journal},
  volume = {20},
  number = {4},
  pages = {364--366},
  issn = {0010-4620},
  doi = {10.1093/comjnl/20.4.364},
  urldate = {2024-01-06},
  abstract = {An improved algorithm for a complete linkage clustering is discussed. The algorithm is based, like the algorithm for the single link cluster method (Slink) presented by Sibson (1973), on a compact representation of a dendrogram: the pointer representation. This approach offers economy in computation. The algorithm is easily programmable.},
  file = {/home/noah/Zotero/storage/HIXEULRS/Defays - 1977 - An efficient algorithm for a complete link method.pdf;/home/noah/Zotero/storage/P3PCCN4I/393966.html}
}

@inproceedings{demmlerABYFrameworkEfficient2015,
  title = {{{ABY}} {\textendash} {{A Framework}} for {{Efficient Mixed-Protocol Secure Two-Party Computation}}},
  booktitle = {{{NDSS Symposium}} 2015},
  author = {Demmler, Daniel and Schneider, Thomas and Zohner, Michael},
  year = {2015},
  month = feb,
  urldate = {2022-12-04},
  abstract = {Secure computation enables multiple mutually distrusting parties to jointly evaluate functions on their private inputs without revealing anything but the result. Generic secure computation protocols in the semi-honest model have been studied extensively and several best practices have evolved. In this work, we design and implement a mixed-protocol framework, called ABY, that efficiently combines secure computation schemes based on Arithmetic sharing, Boolean sharing, and Yao's garbled circuits and that makes available best practice solutions in secure two-party computation. Our framework allows to pre-compute almost all cryptographic operations and provides novel highly efficient conversions between secure computation schemes based on pre-computed oblivious transfer extensions. ABY supports several standard operations and we perform benchmarks on a local network and in a public intercontinental cloud. From our benchmarks we deduce new insights on the efficient design of secure computation protocols, most prominently that oblivious transfer-based multiplications are much more efficient than using homomorphic encryption. We use our framework to construct mixed-protocols for three example applications, private set intersection, biometric matching, and modular exponentiation, and show that they are much more efficient than using a single protocol.},
  langid = {american},
  keywords = {\_tablet},
  file = {/home/noah/Zotero/storage/RF2WU69E/Demmler_et_al_2015_ABY_â€“_A_Framework_for_Efficient_Mixed-Protocol_Secure_Two-Party_Computation.pdf;/home/noah/Zotero/storage/QMXE6AZ6/aby-framework-efficient-mixed-protocol-secure-two-party-computation.html}
}

@article{dunnFuzzyRelativeISODATA1973,
  title = {A {{Fuzzy Relative}} of the {{ISODATA Process}} and {{Its Use}} in {{Detecting Compact Well-Separated Clusters}}},
  author = {Dunn, J. C.},
  year = {1973},
  month = jan,
  journal = {Journal of Cybernetics},
  volume = {3},
  number = {3},
  pages = {32--57},
  publisher = {{Taylor \& Francis}},
  issn = {0022-0280},
  doi = {10.1080/01969727308546046},
  urldate = {2024-01-06},
  abstract = {Two fuzzy versions of the k-means optimal, least squared error partitioning problem are formulated for finite subsets X of a general inner product space. In both cases, the extremizing solutions are shown to be fixed points of a certain operator T on the class of fuzzy, k-partitions of X, and simple iteration of T provides an algorithm which has the descent property relative to the least squared error criterion function. In the first case, the range of T consists largely of ordinary (i.e. non-fuzzy) partitions of X and the associated iteration scheme is essentially the well known ISODATA process of Ball and Hall. However, in the second case, the range of T consists mainly of fuzzy partitions and the associated algorithm is new; when X consists of k compact well separated (CWS) clusters, Xi , this algorithm generates a limiting partition with membership functions which closely approximate the characteristic functions of the clusters Xi . However, when X is not the union of k CWS clusters, the limiting partition is truly fuzzy in the sense that the values of its component membership functions differ substantially from 0 or 1 over certain regions of X. Thus, unlike ISODATA, the ``fuzzy'' algorithm signals the presence or absence of CWS clusters in X. Furthermore, the fuzzy algorithm seems significantly less prone to the ``cluster-splitting'' tendency of ISODATA and may also be less easily diverted to uninteresting locally optimal partitions. Finally, for data sets X consisting of dense CWS clusters embedded in a diffuse background of strays, the structure of X is accurately reflected in the limiting partition generated by the fuzzy algorithm. Mathematical arguments and numerical results are offered in support of the foregoing assertions.}
}

@incollection{dusetzinaOverviewRecordLinkage2014,
  title = {An {{Overview}} of {{Record Linkage Methods}}},
  booktitle = {Linking {{Data}} for {{Health Services Research}}: {{A Framework}} and {{Instructional Guide}} [{{Internet}}]},
  author = {Dusetzina, Stacie B. and Tyree, Seth and Meyer, Anne-Marie and Meyer, Adrian and Green, Laura and Carpenter, William R.},
  year = {2014},
  month = sep,
  publisher = {{Agency for Healthcare Research and Quality (US)}},
  urldate = {2024-01-06},
  abstract = {Randomized controlled trials (RCTs) remain the gold standard for assessing intervention efficacy; however, RCTs are not always feasible or sufficiently timely. Perhaps more importantly, RCT results often cannot be generalized due to a lack of inclusion of ``real-world'' combinations of interventions and heterogeneous patients.1{\textendash}4 With recent advances in information technology, data, and statistical methods, there is tremendous promise in leveraging ever-growing repositories of secondary data to support comparative effectiveness and public health research.2,6,39 While there are many advantages to using these secondary data sources, they are often limited in scope, which in turn limits their utility in addressing important questions in a comprehensive manner. These limitations can be overcome by linking data from multiple sources such as health registries and administrative claims data.7,14{\textendash}16},
  langid = {english},
  file = {/home/noah/Zotero/storage/TWTWIPH6/NBK253312.html}
}

@article{fellegiTheoryRecordLinkage1969a,
  title = {A {{Theory}} for {{Record Linkage}}},
  author = {Fellegi, Ivan P. and Sunter, Alan B.},
  year = {1969},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {64},
  number = {328},
  pages = {1183--1210},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1969.10501049},
  urldate = {2024-01-06},
  abstract = {Abstract A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A 1), a non-link (A 3), and a possible link (A 2). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A 1 when the members of the comparison pair are in fact unmatched, and the error of the decision A 3 when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as and respecti...},
  langid = {english}
}

@inbook{kaufmanDivisiveAnalysisProgram2009,
  title = {6. {{Divisive Analysis}} ({{Program DIANA}})},
  booktitle = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  author = {Kaufman, Leonard and Rousseeuw, Peter},
  year = {2009},
  month = sep,
  pages = {253--279},
  publisher = {{John Wiley \& Sons}},
  collaborator = {Kaufman, Leonard and Rousseeuw, Peter J.},
  googlebooks = {YeFQHiikNo0C},
  isbn = {978-0-470-31748-8},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{kaufmanFindingGroupsData2009a,
  title = {Finding {{Groups}} in {{Data}}: {{An Introduction}} to {{Cluster Analysis}}},
  shorttitle = {Finding {{Groups}} in {{Data}}},
  author = {Kaufman, Leonard and Rousseeuw, Peter J.},
  year = {2009},
  month = sep,
  publisher = {{John Wiley \& Sons}},
  abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists. "Cluster analysis is the increasingly important and practical subject of finding groupings in data. The authors set out to write a book for the user who does not necessarily have an extensive background in mathematics. They succeed very well."{\textemdash}Mathematical Reviews "Finding Groups in Data [is] a clear, readable, and interesting presentation of a small number of clustering methods. In addition, the book introduced some interesting innovations of applied value to clustering literature."{\textemdash}Journal of Classification "This is a very good, easy-to-read, and practical book. It has many nice features and is highly recommended for students and practitioners in various fields of study."{\textemdash}Technometrics An introduction to the practical application of cluster analysis, this text presents a selection of methods that together can deal with most applications. These methods are chosen for their robustness, consistency, and general applicability. This book discusses various types of data, including interval-scaled and binary variables as well as similarity data, and explains how these can be transformed prior to clustering.},
  googlebooks = {YeFQHiikNo0C},
  isbn = {978-0-470-31748-8},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@techreport{krawczykHMACKeyedHashingMessage1997,
  type = {Request for {{Comments}}},
  title = {{{HMAC}}: {{Keyed-Hashing}} for {{Message Authentication}}},
  shorttitle = {{{HMAC}}},
  author = {Krawczyk, Hugo and Bellare, Mihir and Canetti, Ran},
  year = {1997},
  month = feb,
  number = {RFC 2104},
  institution = {{Internet Engineering Task Force}},
  doi = {10.17487/RFC2104},
  urldate = {2023-12-28},
  abstract = {This document describes HMAC, a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key. The cryptographic strength of HMAC depends on the properties of the underlying hash function. This memo provides information for the Internet community. This memo does not specify an Internet standard of any kind},
  file = {/home/noah/Zotero/storage/6427DMFG/Krawczyk et al. - 1997 - HMAC Keyed-Hashing for Message Authentication.pdf}
}

@article{lablansRESTfulInterfacePseudonymization2015,
  title = {A {{RESTful}} Interface to Pseudonymization Services in Modern Web Applications},
  author = {Lablans, Martin and Borg, Andreas and {\"U}ckert, Frank},
  year = {2015},
  month = feb,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {15},
  number = {1},
  pages = {2},
  issn = {1472-6947},
  doi = {10.1186/s12911-014-0123-5},
  abstract = {Medical research networks rely on record linkage and pseudonymization to determine which records from different sources relate to the same patient. To establish informational separation of powers, the required identifying data are redirected to a trusted third party that has, in turn, no access to medical data. This pseudonymization service receives identifying data, compares them with a list of already reported patient records and replies with a (new or existing) pseudonym. We found existing solutions to be technically outdated, complex to implement or not suitable for internet-based research infrastructures. In this article, we propose a new RESTful pseudonymization interface tailored for use in web applications accessed by modern web browsers.}
}

@article{laudPrivacypreservingRecordLinkage2018,
  title = {Privacy-Preserving Record Linkage in Large Databases Using Secure Multiparty Computation},
  author = {Laud, Peeter and Pankova, Alisa},
  year = {2018},
  month = oct,
  journal = {BMC Medical Genomics},
  volume = {11},
  number = {4},
  pages = {33--46},
  publisher = {{BioMed Central}},
  issn = {1755-8794},
  doi = {10.1186/s12920-018-0400-8},
  urldate = {2024-01-07},
  abstract = {Practical applications for data analysis may require combining multiple databases belonging to different owners, such as health centers. The analysis should be performed without violating privacy of neither the centers themselves, nor the patients whose records these centers store. To avoid biased analysis results, it may be important to remove duplicate records among the centers, so that each patient's data would be taken into account only once. This task is very closely related to privacy-preserving record linkage. This paper presents a solution to privacy-preserving deduplication among records of several databases using secure multiparty computation. It is build upon one of the fastest practical secure multiparty computation platforms, called Sharemind. The tests on ca 10 million records of simulated databases with 1000 health centers of 10000 records each show that the computation is feasible in practice. The expected running time of the experiment is ca. 30 min for computing servers connected over 100 Mbit/s WAN, the expected error of the results is 2-40, and no errors have been detected for the particular test set that we used for our benchmarks. The solution is ready for practical use. It has well-defined security properties, implied by the properties of Sharemind platform. The solution assumes that exact matching of records is required, and a possible future research would be extending it to approximate matching.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {/home/noah/Zotero/storage/IKYET2MD/Laud and Pankova - 2018 - Privacy-preserving record linkage in large databas.pdf}
}

@inproceedings{lazrigPrivacyPreservingProbabilistic2018,
  title = {Privacy {{Preserving Probabilistic Record Linkage Without Trusted Third Party}}},
  booktitle = {2018 16th {{Annual Conference}} on {{Privacy}}, {{Security}} and {{Trust}} ({{PST}})},
  author = {Lazrig, Ibrahim and Ong, Toan C. and Ray, Indrajit and Ray, Indrakshi and Jiang, Xiaoqian and Vaidya, Jaideep},
  year = {2018},
  month = aug,
  pages = {1--10},
  doi = {10.1109/PST.2018.8514192},
  urldate = {2024-01-07},
  abstract = {For the purpose of research, organizations often need to share and link data belonging to a single individual while protecting her privacy. This problem, referred to as privacy preserving record linkage (PPRL), has been investigated by researchers. Most PPRL works focus on deterministic linkages where the identifying attributes of two records must be equal in order to declare them to belong to the same individual. Moreover, most of these methods require the active participation of a trusted third party (TTP). If this TTP is compromised, it makes the data from all participating parties vulnerable to information leakage. The proposed work improves upon the existing methods in two ways. First, we propose a protocol which does not require two records to have an exact match on identifying attributes in order to be declared as belonging to the same individual. Second, we investigate probabilistic PPRL in the two-party setting without resorting to any TTP. We use Bloom filters for probabilistic matching and Yao's garbled circuit to perform the computation needed for the matching on encrypted data. To alleviate the computation and communication overhead of Yao's protocol, we leverage data blocking methods and optimize the computation. We provide a security proof of our method and experimentally evaluate the performance gained on large benchmark datasets.},
  file = {/home/noah/Zotero/storage/6YPVCKPK/8514192.html}
}

@article{lloydLeastSquaresQuantization1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2024-01-06},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\^bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {/home/noah/Zotero/storage/VDWZQLCQ/Lloyd - 1982 - Least squares quantization in PCM.pdf;/home/noah/Zotero/storage/YJUBSNGG/1056489.html}
}

@article{schmidtmannQualityRecordLinkage2016,
  title = {{Quality of record linkage in a highly automated cancer registry that relies on encrypted identity data}},
  author = {Schmidtmann, Irene and Sariyar, Murat and Borg, Andreas and {Gerold-Ay}, Aslihan and Heidinger, Oliver and Hense, Hans-Werner and Krieg, Volker and Hammer, Ga{\"e}l Paul},
  year = {2016},
  month = jun,
  journal = {GMS Medizinische Informatik, Biometrie und Epidemiologie},
  volume = {12},
  number = {1},
  pages = {Doc02},
  publisher = {{German Medical Science GMS Publishing House}},
  issn = {1860-9171},
  doi = {10.3205/mibe000164},
  abstract = {Ziel der Arbeit: Krankheitsregister in Deutschland und einigen anderen L{\"a}ndern sind darauf angewiesen, mehrere Meldungen zu einem Patienten anhand von Identit{\"a}tsdaten zusammenzuf{\"u}hren, da keine eindeutige Personenkennung verf{\"u}gbar ist. Diese Identit{\"a}tsdaten werden h{\"a}ufig aus Datenschutzgr{\"u}nden pseudonymisiert. Einige Fehler beim Record Linkage sind unvermeidbar. In der vorliegenden Arbeit werden sie f{\"u}r das Epidemiologische Krebsregister Nordrhein-Westfalen, das chiffrierte Identit{\"a}tsdaten zum Record Linkage verwendet, quantifiziert. Methoden: Eine Stichprobe von Meldungen an das Epidemiologische Krebsregister Nordrhein-Westfalen, die mit der Information {\"u}ber die Zuordnung zu einer Person versehen war, wurde gezogen. Parallel dazu wurden Klartextidentit{\"a}tsdaten f{\"u}r diese Meldungen durch Dechiffrierung wiedergewonnen, um einen Gold-Standard zu erzeugen. Die Zuordnungsfehlerh{\"a}ufigkeiten wurden durch Vergleich der Ergebnisse des Routine-Record Linkage mit dem Gold-Standard bestimmt. Die Fehlerraten wurden f{\"u}r umfangreichere Register hochgerechnet.Ergebnisse: In der untersuchten Stichprobe betrug die Homonymfehlerrate 0.015\%, die Synonymfehlerrate 0.2\%, das F-Ma{\ss} ergab 0.9921. Eine Hochrechnung auf umfangreichere Datenbanken ergab, dass bei realistischen Annahmen {\"u}ber die Zunahme des Umfangs eine Homonymfehlerrate von etwa 1\% und eine Synonymfehlerrate von etwa 2\% zu erwarten sind.Schlussfolgerung: Die beobachteten Fehlerraten sind niedrig, darin zeigt sich, dass effektive Methoden zur Standardisierung und Sicherung der Datenqualit{\"a}t implementiert wurden. Dies ist essenziell, damit die Fehlerraten niedrig bleiben, wenn der Umfang des Registers zunimmt. Durch den geplanten Einschluss der eindeutigen Krankenversicherungsnummer ist eine weitere Verbesserung der G{\"u}te des Record Linkage zu erwarten. Krebsregistrierung, die allein auf elektronische Meldungen basiert, kann gro{\ss}e Datenmengen verarbeiten bei hoher Qualit{\"a}t des Record Linkage.},
  copyright = {This is an Open Access article distributed under the terms of the Creative Commons Attribution 4.0 License.},
  langid = {engl},
  keywords = {cancer registry,data quality,encrypted data,evaluation,record linkage},
  file = {/home/noah/Zotero/storage/FGJSQSSA/Schmidtmann et al. - 2016 - Quality of record linkage in a highly automated ca.pdf;/home/noah/Zotero/storage/9BPDKQRJ/mibe000164.html}
}

@article{stammlerMainzellisteSecureEpiLinkerMainSEL2022,
  title = {Mainzelliste {{SecureEpiLinker}} ({{MainSEL}}): Privacy-Preserving Record Linkage Using Secure Multi-Party Computation},
  shorttitle = {Mainzelliste {{SecureEpiLinker}} ({{MainSEL}})},
  author = {Stammler, Sebastian and Kussel, Tobias and Schoppmann, Phillipp and Stampe, Florian and Tremper, Galina and Katzenbeisser, Stefan and Hamacher, Kay and Lablans, Martin},
  year = {2022},
  month = mar,
  journal = {Bioinformatics},
  volume = {38},
  number = {6},
  pages = {1657--1668},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa764},
  urldate = {2023-01-04},
  abstract = {Record Linkage has versatile applications in real-world data analysis contexts, where several datasets need to be linked on the record level in the absence of any exact identifier connecting related records. An example are medical databases of patients, spread across institutions, that have to be linked on personally identifiable entries like name, date of birth or ZIP code. At the same time, privacy laws may prohibit the exchange of this personally identifiable information (PII) across institutional boundaries, ruling out the outsourcing of the record linkage task to a trusted third party. We propose to employ privacy-preserving record linkage (PPRL) techniques that prevent, to various degrees, the leakage of PII while still allowing for the linkage of related records.We develop a framework for fault-tolerant PPRL using secure multi-party computation with the medical record keeping software Mainzelliste as the data source. Our solution does not rely on any trusted third party and all PII is guaranteed to not leak under common cryptographic security assumptions. Benchmarks show the feasibility of our approach in realistic networking settings: linkage of a patient record against a database of 10~000 records can be done in 48\,s over a heavily delayed (100\,ms) network connection, or 3.9\,s with a low-latency connection.The source code of the sMPC node is freely available on Github at https://github.com/medicalinformatics/SecureEpilinker subject to the AGPLv3 license. The source code of the modified Mainzelliste is available at https://github.com/medicalinformatics/MainzellisteSEL.Supplementary data are available at Bioinformatics online.},
  keywords = {\_tablet},
  file = {/home/noah/Zotero/storage/9ADU4MVD/Stammler_et_al_2022_Mainzelliste_SecureEpiLinker_(MainSEL) (suppl).pdf;/home/noah/Zotero/storage/JUK93ILL/Stammler_et_al_2022_Mainzelliste_SecureEpiLinker_(MainSEL).pdf;/home/noah/Zotero/storage/PV96FEI3/5900257.html}
}

@inproceedings{tranGeCoOnlinePersonal2013,
  title = {{{GeCo}}: An Online Personal Data Generator and Corruptor},
  shorttitle = {{{GeCo}}},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Information}} \& {{Knowledge Management}}},
  author = {Tran, Khoi-Nguyen and Vatsalan, Dinusha and Christen, Peter},
  year = {2013},
  month = oct,
  series = {{{CIKM}} '13},
  pages = {2473--2476},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2505515.2508207},
  urldate = {2023-10-12},
  abstract = {We demonstrate GeCo, an online personal data GEnerator and COrruptor that facilitates the creation of realistic personal data ranging from names, addresses, and dates, to social security and credit card numbers, as well as numerical values such as salary or blood pressure. Using an intuitive Web interface, a user can create records containing such data according to their needs, and apply various corruption functions to generate duplicates of these records. Synthetic personal data are increasingly required in areas such as record de-duplication, fraud detection, cloud computing, and health informatics, where data quality issues can significantly affect the outcomes of data integration, processing, and mining projects. Privacy concerns, however, often make it difficult for researchers to obtain real data that contain personal details. Compared to other data generators that have to be downloaded, installed and customized,GeCo allows the creation of personal data with much less effort. In this demonstration we show (1) how different types of attributes, and dependencies between them, can be specified; (2) how the generated data can be modified using various types of corruption functions; and (3) how a user can contribute to GeCo by providing attribute generation functions and look-up files. We believe GeCo will be a valuable tool for researchers that require realistic personal data to evaluate their algorithms with regard to efficiency and effectiveness.},
  isbn = {978-1-4503-2263-8},
  keywords = {data generation,duplicates,online demo,synthetic data},
  file = {/home/noah/Zotero/storage/GLIF8LI2/Tran et al. - 2013 - GeCo an online personal data generator and corrup.pdf}
}

@article{xuComprehensiveSurveyClustering2015,
  title = {A {{Comprehensive Survey}} of {{Clustering Algorithms}}},
  author = {Xu, Dongkuan and Tian, Yingjie},
  year = {2015},
  month = jun,
  journal = {Annals of Data Science},
  volume = {2},
  number = {2},
  pages = {165--193},
  issn = {2198-5812},
  doi = {10.1007/s40745-015-0040-1},
  urldate = {2024-01-06},
  abstract = {Data analysis is used as a common method in modern science research, which is across communication science, computer science and biology science. Clustering, as the basic composition of data analysis, plays a significant role. On one hand, many tools for cluster analysis have been created, along with the information increase and subject intersection. On the other hand, each clustering algorithm has its own strengths and weaknesses, due to the complexity of information. In this review paper, we begin at the definition of clustering, take the basic elements involved in the clustering process, such as the distance or similarity measurement and evaluation indicators, into consideration, and analyze the clustering algorithms from two perspectives, the traditional ones and the modern ones. All the discussed clustering algorithms will be compared in detail and comprehensively shown in Appendix Table~22.},
  langid = {english},
  keywords = {Clustering,Clustering algorithm,Clustering analysis,Survey,Unsupervised learning},
  file = {/home/noah/Zotero/storage/RM2AC5JB/Xu and Tian - 2015 - A Comprehensive Survey of Clustering Algorithms.pdf}
}

@inproceedings{zhangBIRCHEfficientData1996,
  title = {{{BIRCH}}: An Efficient Data Clustering Method for Very Large Databases},
  shorttitle = {{{BIRCH}}},
  booktitle = {Proceedings of the 1996 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Zhang, Tian and Ramakrishnan, Raghu and Livny, Miron},
  year = {1996},
  month = jun,
  series = {{{SIGMOD}} '96},
  pages = {103--114},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/233269.233324},
  urldate = {2024-01-06},
  abstract = {Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.},
  isbn = {978-0-89791-794-0},
  file = {/home/noah/Zotero/storage/28BULLJU/Zhang et al. - 1996 - BIRCH an efficient data clustering method for ver.pdf}
}
